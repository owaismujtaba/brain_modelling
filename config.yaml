experiment:
  name: brain-2-text
  description: Text decoding from brain signals
  version: 1.0.0

logging:
  name: 'training'
  level: INFO
  log_file_path: 'logs/training.log'

run: 'train'

dataset:
  info:
    dataset_dir: 'C:\Users\Investigador\work\nejm-brain-to-text\data'
    bad_trials_dict: None
    seed: -1
  data_transforms:
    white_noise_std: 1.0
    constant_offset_std: 0.2
    random_walk_std: 0.0
    random_walk_axis: -1
    static_gain_std: 0.0
    random_cut: 3
    smooth_kernel_size: 100
    smooth_data: true
    smooth_kernel_std: 2

train:
  init_from_checkpoint: false
  init_checkpoint_path: ''

  test_percentage: 0
  batch_size: 64
  n_train_batches: 120000
  days_per_batch: 4
  must_include_days: []
  feature_subset: []
  pin_memory: true
  shuffle_data: false
  device: 'GPU'
  n_data_workers: 4

  num_training_batches: 120000
  lr_scheduler_type: cosine
  lr_max: 0.005
  lr_min: 0.0001
  lr_decay_steps: 120000
  lr_warmup_steps: 1000
  lr_max_day: 0.005
  lr_min_day: 0.0001
  lr_decay_steps_day: 120000
  lr_warmup_steps_day: 1000
  
  beta0: 0.9
  beta1: 0.999
  epsilon: 1e-8
  weight_decay: 0.001
  weight_decay_day: 0
  seed: 10
  grad_norm_clip_value: 10

  batches_per_train_log: 200 # number of batches per training log
  batches_per_val_step: 2000 

model:
  name: 'GRU'
  neural_dim: 512
  max_seq_elements: 500
  n_classes: 41
  patch_size: 14 # size of the input patches (14 time steps)
  patch_stride: 4 # stride for the input patches (4 time steps)


directory:
  output_dir: "trained_models/${model.name}"
  checkpoint_dir: "trained_models/${model.name}/checkpoint"
