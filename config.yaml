experiment:
  name: brain-2-text
  description: Text decoding from brain signals
  version: 1.0.0

run: 'create_llm_dataset' # train/inference
create_llm_dataset: true

logging:
  name: 'create_llm_dataset'
  level: INFO
  log_file_path: 'logs/${run}.log'
  log_per_by_day: true



dataset:
  info:
    dataset_dir: 'C:\Users\Investigador\work\brain_modelling\data\data\raw\hdf5_data_final'
    bad_trials_dict: None
    seed: -1
    description_filepath: 'data/raw/t15_copyTaskData_description.csv'

  data_transforms:
    white_noise_std: 1.0
    constant_offset_std: 0.2
    random_walk_std: 0.0
    random_walk_axis: -1
    static_gain_std: 0.0
    random_cut: 3
    smooth_kernel_size: 100
    smooth_data: true
    smooth_kernel_std: 2

model:
  name: 'GRU_Attention'
  neural_dim: 512
  n_units: 768
  n_layers: 7
  max_seq_elements: 500
  n_days: 45
  input_dropout: 0.1
  rnn_dropout: 0.2
  n_classes: 41
  patch_size: 14 # size of the input patches (14 time steps)
  patch_stride: 4 # stride for the input patches (4 time steps)

  hidden_fc: 256
  n_resblocks: 2
  attn_heads: 4
  attn_dropout: 0.1
  sequence_output: True

train:
  init_from_checkpoint: false
  init_checkpoint_path: '/home/owais/Desktop/work/brain_modelling/trained_models/GRU/checkpoint/best_model'

  test_percentage: 0
  batch_size: 16
  n_train_batches: 120000
  days_per_batch: 4
  must_include_days: []
  feature_subset: []
  pin_memory: true
  shuffle_data: false
  device: 'GPU'
  n_data_workers: 4

  num_training_batches: 120000
  lr_scheduler_type: cosine
  lr_max: 0.0005
  lr_min: 0.0001
  lr_decay_steps: 120000
  lr_warmup_steps: 1000
  lr_max_day: 0.005
  lr_min_day: 0.0001
  lr_decay_steps_day: 120000
  lr_warmup_steps_day: 1000
  
  beta0: 0.9
  beta1: 0.999
  epsilon: 1e-8
  weight_decay: 0.001
  weight_decay_day: 0
  seed: 10
  grad_norm_clip_value: 10

  batches_per_train_log: 200 # number of batches per training log
  batches_per_val_step: 1000 

  checkpoint_save_interval: 2000

directory:
  output_dir: "trained_models/${model.name}"
  checkpoint_dir: "trained_models/${model.name}/checkpoint"
  llm_dataset_dir: 'llm_data'
